{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Collab link: https://colab.research.google.com/drive/1Sn8uAyPe6Uh1rbhXGKfPH0CrQwCVTzSc?usp=sharing\n",
        "\n",
        "# Scientific context of the problem\n",
        "\n",
        "High-energy physics, and more particularly the particle physics field, aims at establishing the elementary constituents of matter. This is usually achieved through protons and antiprotons collisions experiments in circular or linear accelerators to produce new, or \"exotic\" particles. The low ratio of creation of these new particles ($300$ for $10^{11}$ collisions in the case of the Higgs Boson for instance), and their quick decay into a different particle make their detection a very non-trivial task.\n",
        "\n",
        "In this sense, and given the limited amount of data together with their expensive nature (both in terms of money and time), it is crucial to detect accurately true detections from noisy, background ones.\n",
        "\n",
        "To this end, machine learning algorithms have been shown useful during the past decade and this notebook aims at developping and analysing some ML approaches to solve this binary (or two-class) classification problem.\n",
        "\n",
        "**Reference**: this problem is based on [Baldi et al., Searching for exotic particles in high-energy physics with deep learning, Nature, 2014](https://www.nature.com/articles/ncomms5308)."
      ],
      "metadata": {
        "id": "RtFIen19WOu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The data\n",
        "\n",
        "In this problem, we consider a subset of the [SUSY](https://archive.ics.uci.edu/dataset/279/susy#) dataset (you don't need to download it). It consists in Monte-Carlo simulated data aiming at benchmarking algorithms to differentiate between collisions creating symmetric (exotic) particles and collisions creating standard model particles.\n",
        "\n",
        "The data are arranged in 8 features representing kinematic properties measured by the accelerator sensors. Based on these features, the aim is to implement and compare several machine learning models.\n",
        "\n",
        "1. What is the machine learning family this problem is about? What is the nature of the input data?\n",
        "\n",
        "2. Enumerate some methods that could be used for such a task."
      ],
      "metadata": {
        "id": "cHEsjsf4DMq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites\n",
        "\n",
        "3. Import the necessary packages, in particular, we will use:\n",
        "> * [`matplotlib`](https://matplotlib.org/) for data visualisation and plots,\n",
        "> * [`numpy`](https://numpy.org/) for standard numerical operations and algebra,\n",
        "> * [`pandas`](https://pandas.pydata.org/) for data analysis,\n",
        "> * [`scikit-learn`](https://scikit-learn.org/stable/) for ML algorithms and related functions used for model selection or data pre-processing."
      ],
      "metadata": {
        "id": "5tb5ItMEVYEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xj4fjZv1VXRx"
      },
      "outputs": [],
      "source": [
        "import pandas                       # For data management\n",
        "import matplotlib.pyplot as plt     # For plotting\n",
        "import matplotlib as mpl            # For plotting setup\n",
        "import numpy as np                  # For numerical calculations\n",
        "import sklearn                      # Machine learning library\n",
        "from sklearn import model_selection # For splitting the data into train/test\n",
        "from sklearn import tree            # For decision trees\n",
        "\n",
        "# Formatting the plots\n",
        "plt.rcParams['figure.figsize'] = [6,6]\n",
        "plt.rcParams['font.size'] = 18\n",
        "plt.rcParams['font.weight'] = 'normal'\n",
        "plt.style.use('default')\n",
        "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
        "mpl.rcParams['mathtext.rm'] = 'serif'\n",
        "mpl.rcParams['font.size'] = 22\n",
        "mpl.rcParams['axes.formatter.limits'] = (-6, 6)\n",
        "mpl.rcParams['axes.formatter.use_mathtext'] = True\n",
        "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
        "mpl.rcParams['mathtext.rm'] = 'Bitstream Vera Sans'\n",
        "mpl.rcParams['mathtext.it'] = 'Bitstream Vera Sans:italic'\n",
        "mpl.rcParams['mathtext.bf'] = 'Bitstream Vera Sans:bold'\n",
        "mpl.rcParams['xtick.minor.visible'] = True\n",
        "mpl.rcParams['ytick.minor.visible'] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. To download the subset of the SUSY dataset we'll use in this notebook, you can download it using the following code, or by downloading it from [my Github](https://github.com/tbonnair/Machine-Learning-Principles-with-Applications-in-Physics). The filename is `SUSY_subset_f1.csv.zip` that you'll have to unzip."
      ],
      "metadata": {
        "id": "sqcmy56HV-Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tbonnair/Courses.git/\n",
        "!unzip -q  'Courses/Machine-Learning-Principles-with-Applications-in-Physics/2025-2026/Hands-on/data/SUSY_subset_f1.csv.zip'"
      ],
      "metadata": {
        "id": "4DApyDnLWA9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef48aba-3090-412c-9ac5-adf8f8291489"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Courses'...\n",
            "remote: Enumerating objects: 145, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 145 (delta 0), reused 2 (delta 0), pack-reused 139 (from 1)\u001b[K\n",
            "Receiving objects: 100% (145/145), 123.54 MiB | 21.73 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading, exploring, and preparing the data\n",
        "\n",
        "5. Load the data using the `pandas` dataset."
      ],
      "metadata": {
        "id": "HpPQ4X47WDZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = 'SUSY_subset_f1.csv'\n",
        "dataset = pandas.read_csv(datapath)"
      ],
      "metadata": {
        "id": "ShFaJmILWSbd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explore the data (how many events are there, what are the features, etc.) using the `pandas` library. What is the name of the key we want to predict? To access a given key (column), you can use `dataset['keyname']`."
      ],
      "metadata": {
        "id": "FqpvhFw5WhNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to explore the data"
      ],
      "metadata": {
        "id": "kXzcNvSuWw5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "ofyE-kRD7YiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Check the proportion of events in the two classes."
      ],
      "metadata": {
        "id": "fZG1HayxXPH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "c_NXqGZjYz7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Arange the data into a feature matrix $\\boldsymbol{X}$ and a target vector $\\boldsymbol{y}$. Then, prepare the dataset and split it into 80% training and 20% test using the scikit-learn [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. **Remind the role of the train and test sets.**"
      ],
      "metadata": {
        "id": "uA8K330XXnul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "N_r9PNdtX-d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A first model: decision tree\n",
        "\n",
        "Now the data are ready to be used for training, let us build a first model: a decision tree.\n",
        "\n",
        "9. Using the scikit-learn documentation, find how to fit a Decision Tree with a maximum depth of 15 and using the entropy loss on the training dataset."
      ],
      "metadata": {
        "id": "CjEIkAXbcnML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "3owrpRHtdITu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Test your model on the test dataset and look at the obtained score."
      ],
      "metadata": {
        "id": "BhD_Tun_eZuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for the prediction on the test set"
      ],
      "metadata": {
        "id": "2CbievL9eZf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Tune the hyperparameter of the maximum depth. To this end, split the training data again into two parts: a training dataset and a validation dataset. Then, find the hyperparameter having the best score on the validation set with depth in the range [1, 20]."
      ],
      "metadata": {
        "id": "4IVVeODoeuud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "dAOeV8GFgtym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Intepret the evolution of the validation score when varying the maximum depth: how does the validation error behaves and why? (To help you, you can also store and visualise the evolution of the training error in addition to validation error.)"
      ],
      "metadata": {
        "id": "vU_Y_y5WjLCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional: You can also explore how your decision tree is classifying your data by using the [`plot_tree`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) function from scikit-learn."
      ],
      "metadata": {
        "id": "P0P3GZWforIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "ccHbic3BHLgt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving the model with more trees: random forest\n",
        "\n",
        "13. Remind the principle behind random forests.\n",
        "\n",
        "\n",
        "14. Using the scikit-learn documentation, fit a random forest to the training data with a number of trees of 20 and a maximum depth of 15. Compare this setup with our first decision tree."
      ],
      "metadata": {
        "id": "k6ErNvwgg7y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your random forest fit"
      ],
      "metadata": {
        "id": "zymb004tlIJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Repeat the hyperparameter tuning for tuning the number of trees in [1, 50] at fixed maximum depth = 15 to find the best random forest model for our data."
      ],
      "metadata": {
        "id": "tQs1FtY7k_Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your tuning of the number of trees"
      ],
      "metadata": {
        "id": "5K8BxGdYpJLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Does the curve showing the evolution of the test error look the same as previously with the decision tree depth? Why?\n",
        "\n",
        "17. What other tree-based method could you use to solve this classification task? Check it out on scikit-learn."
      ],
      "metadata": {
        "id": "YUqZ_GRTx32Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importance of hand-crafted features\n",
        "\n",
        "For such models that do not learn new features from the data, hand-crafted can matter a lot for the performance.\n",
        "This is in fact what the authors releasing the SUSY dataset did by providing additionally 10 hand-crafted features that are functions of the eight first obtained directly from the sensors.\n",
        "\n",
        "18. Download the new dataset and explore the new data as before.\n"
      ],
      "metadata": {
        "id": "kk6WtTUWjocs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapath2 = 'SUSY_subset2_all_features.csv'\n",
        "!unzip -q  'Courses/Machine-Learning-Principles-with-Applications-in-Physics/2025-2026/Hands-on/data/SUSY_subset2_all_features.csv.zip'\n",
        "\n",
        "dataset2 = pandas.read_csv(datapath2)"
      ],
      "metadata": {
        "id": "yNpAiuwQCkbF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset2.info())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ASQfjGDFWcAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2edf03-916c-49ac-a7a6-87141b516075"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 19 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   0       100000 non-null  float64\n",
            " 1   1       100000 non-null  float64\n",
            " 2   2       100000 non-null  float64\n",
            " 3   3       100000 non-null  float64\n",
            " 4   4       100000 non-null  float64\n",
            " 5   5       100000 non-null  float64\n",
            " 6   6       100000 non-null  float64\n",
            " 7   7       100000 non-null  float64\n",
            " 8   8       100000 non-null  float64\n",
            " 9   9       100000 non-null  float64\n",
            " 10  10      100000 non-null  float64\n",
            " 11  11      100000 non-null  float64\n",
            " 12  12      100000 non-null  float64\n",
            " 13  13      100000 non-null  float64\n",
            " 14  14      100000 non-null  float64\n",
            " 15  15      100000 non-null  float64\n",
            " 16  16      100000 non-null  float64\n",
            " 17  17      100000 non-null  float64\n",
            " 18  18      100000 non-null  float64\n",
            "dtypes: float64(19)\n",
            "memory usage: 14.5 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Fit a random forest on the full set of features and compare the obtained results with the previous features."
      ],
      "metadata": {
        "id": "HhN9Lymo5rpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your random forest fit on the whole set of features"
      ],
      "metadata": {
        "id": "5ASCaWagTZ-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Going further (if time)"
      ],
      "metadata": {
        "id": "tk133qax5N0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed-forward neural network\n",
        "As said before, hand-crafted features can help but require expertise and knowledge to be built. One way to extract interesting features automatically from the first 8 could be to train a feed-forward neural network and see if it can outperform the results of the (already satisfactory) random forest.\n",
        "\n",
        "To do so, we can use [`PyTorch`](https://pytorch.org/)  and implement a small feed forward neural networks."
      ],
      "metadata": {
        "id": "YAFACoShtmwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Import the necessary packages from PyTorch"
      ],
      "metadata": {
        "id": "GbzizfAvW-Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Import and setup\n",
        "\n",
        "# Neural network useful packages from PyTorch and sklearn\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from sklearn.utils import shuffle # For shuffling the data\n",
        "\n",
        "device_cuda = 'cpu'  # We'll use the CPU for the computation\n",
        "\n",
        "labels_features = dataset.keys()[1::]   # Labels of the feature in the dataset\n",
        "label_pred = dataset.keys()[0]          # Label of the predicition\n",
        "\n",
        "X = dataset[labels_features]\n",
        "y = dataset[label_pred]\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=102)"
      ],
      "metadata": {
        "id": "bvjvH-w3VI57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before doing anything, we need to recast the data into a PyTorch format and standardize them."
      ],
      "metadata": {
        "id": "zaqLq4-wgzIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Standardization\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1,1)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1,1)\n",
        "\n",
        "means = X_train_tensor.mean(dim=1, keepdim=True)\n",
        "stds = X_train_tensor.std(dim=1, keepdim=True)\n",
        "X_train_tensor = (X_train_tensor - means) / stds\n",
        "\n",
        "means = X_test_tensor.mean(dim=1, keepdim=True)\n",
        "stds = X_test_tensor.std(dim=1, keepdim=True)\n",
        "X_test_tensor = (X_test_tensor - means) / stds\n",
        "\n",
        "print(f'Mean = {X_train_tensor.mean()}')\n",
        "print(f'Std = {X_train_tensor.std()}')"
      ],
      "metadata": {
        "id": "OJONKuQ7g7Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, a model can be specified as a sequence of several layers. All the possible layers proposed by PyTorch are listed [here](https://pytorch.org/docs/stable/nn.html#linear-layers).\n",
        "\n",
        "To implement a new model, one needs to create an object `nn.Sequential` and specify the layers successively with their input/output sizes.\n",
        "\n",
        "20. Specify a network with 3 hidden layers, ReLU activations, and 100 hidden neurons in each hidden layers. The final layer will be of size one with a Sigmoid activation function. To help you, you can draw the diagram of this neural network."
      ],
      "metadata": {
        "id": "G2IY1dxhXVVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "neural_net = nn.Sequential(\n",
        "    # Build your layers\n",
        ")\n",
        "\n",
        "#print(neural_net)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TlLnn2-OXT1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note there is a cleaner way of defining the neural network models in PyTorch by defining a subclass of `nn.Module` and creating two functions: an `__init__(self)` function specifying the layers, and a `forward(self)` function specifying how a data is propagating throughout the network. You can have a look at [this link](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)."
      ],
      "metadata": {
        "id": "P6nJorSUcv4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Specify a loss function and an optimization procedure. In our case, we'll use the binary cross-entropy loss together with the [`SGD optimizer`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html). Set the learning rate to 0.02."
      ],
      "metadata": {
        "id": "mvpOOJRbcvge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "loss_fn = nn.BCELoss()  # binary cross entropy loss function\n",
        "optimizer = optim.SGD(neural_net.parameters(), lr=0.02)"
      ],
      "metadata": {
        "id": "FlfzgYGDdlDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train our neural network, we use the backpropagation algorithm and the stochastic gradient descent method.\n",
        "\n",
        "22. Remind how stochastic gradient descent works.\n",
        "\n",
        "23. Train the neural network for 20 epochs using a batch size of 32. To do this, you can use two loops, one for the epochs, one for the batches.\n",
        "To compute and update the gradient, there are four steps:\n",
        "> * First compute the loss on the batch,\n",
        "> * Clean your optimizer using the [`optimizer.zero_grad()`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html) function,\n",
        "> * Call the [`backward()`](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) function of the loss to compute the gradient,\n",
        "> * Make a step in the opposite gradient direction using the [`optimizer.step()`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html) function.\n",
        "\n",
        "Note: to shuffle the dataset, you can use the [`shuffle`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html) method from scikit-learn."
      ],
      "metadata": {
        "id": "0XOFGD8-d3J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "n_epochs = 20\n",
        "batch_size = 32\n",
        "n_batches = int(len(X_train_tensor) / batch_size)\n",
        "print(n_batches)\n",
        "\n",
        "# Your code for optimization"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KqbYOvSVd-gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that in 20 epochs with a simple feed-forward neural networks made of 3 hidden layers, we achieve a slightly poorer performance than the random forest applied on the set of basic features.\n",
        "\n",
        "This shows that the feed-forward neural network needs some fine-tuning to perform better. To explore that, you can play with the code and tune hyperparameters, add fancy tuning like dropping, weight decay, momentum, or regularization.\n",
        "\n",
        "You can also go and use the full dataset provided in the seminal papers which is in fact made of millions of events, not just $10^5$ that could help your neural network converge to better solutions."
      ],
      "metadata": {
        "id": "3TD_5pnQl9T1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Randomized search for hyperparameter tuning\n",
        "In our examples, we always focus on one hyperparameter at a time. In fact, using scikit-learn you can specify several ranges for your hyperparameters and test them randomly to find the association of parameters using randomized search or grid search cross-validation. Randomized search is more efficient numerically but is not leading you to the best possible set of parameters, while grid search consists in an exhaustive search in your pre-defined ranges for parameters. Here we illustrate it with randomized search."
      ],
      "metadata": {
        "id": "dtqfbpt5U8fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# define range of values for each hyperparameter\n",
        "hyperparameters_range = [{\n",
        "        \"n_estimators\": [1, 5, 10, 20, 40],       # Number of trees\n",
        "        \"max_features\": [0.5, 1.0, \"sqrt\", \"log2\"],  # Proportion of the features used for training you RFs\n",
        "        \"max_depth\": [5, 10, 15, 20],  # Maximum depth of the trees trained in RFs\n",
        "    }]\n",
        "\n",
        "# Create a base model\n",
        "RF = RandomForestClassifier(n_estimators=20, max_depth=15, n_jobs=-1)\n",
        "\n",
        "# Random search with cross validation\n",
        "RF_CV = RandomizedSearchCV(\n",
        "    RF, param_distributions=hyperparameters_range, cv=5)\n",
        "\n",
        "# Fit the search\n",
        "RF_CV.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "26KLjHfMtnWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now select the best model and test it."
      ],
      "metadata": {
        "id": "TTWduVq1xKTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "print(RF_CV.best_params_)         # Prints the best parameters\n",
        "RF_best = RF_CV.best_estimator_   # Select the best model\n",
        "RF_best.score(X_test, y_test)     # Test dataset"
      ],
      "metadata": {
        "id": "FsOC4p_nw75x",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could do similarly with the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) function from scikit-learn to perform grid search cross-validation for tuning your hyperparameters."
      ],
      "metadata": {
        "id": "EP-12v-DWDZP"
      }
    }
  ]
}